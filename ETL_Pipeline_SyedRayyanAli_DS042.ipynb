{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W8t5ShT181j",
        "outputId": "034a6daa-17b4-4d5b-ad99-956cd21918f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.11.3)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas requests kagglehub[pandas-datasets] pymongo schedule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from pymongo import MongoClient\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "import schedule\n",
        "import time"
      ],
      "metadata": {
        "id": "1334_PcZ2I4S"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ETLPipeline:\n",
        "    \"\"\"\n",
        "    A class for collecting, transforming, analyzing, and loading financial stock data\n",
        "    from MarketStack API and Kaggle datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    # API and configuration constants\n",
        "    ticker_url = 'https://gist.githubusercontent.com/rayyanali00/a311644d6d902100242345d1198a7a53/raw/tickers.json'\n",
        "    marketstack_baseapiurl = 'https://api.marketstack.com/v2/eod?access_key=580c67effd676378137d18d392f35603'\n",
        "    marketstack_apiurlgist = 'https://gist.githubusercontent.com/rayyanali00/ec7fa991d7bb93d51a786ae811563ebc/raw/marketstack_stockdata.json'\n",
        "    datefrom = '2025-03-01'\n",
        "    dateto = '2025-04-01'\n",
        "    IS_MOCK = True\n",
        "\n",
        "    def __init__(self, datefrom=None, dateto=None):\n",
        "        \"\"\"Initialize DataCollection with optional custom date range.\"\"\"\n",
        "        if datefrom and dateto:\n",
        "            self.datefrom = datefrom\n",
        "            self.dateto = dateto\n",
        "\n",
        "    def api_request(self, requesturl):\n",
        "        \"\"\"Generic method to make API requests.\"\"\"\n",
        "        data = requests.get(requesturl)\n",
        "        return data.json()\n",
        "\n",
        "    def get_tickers(self):\n",
        "        \"\"\"Fetch the list of tickers from the provided ticker URL.\"\"\"\n",
        "        tickers_json = self.api_request(self.ticker_url)\n",
        "        data = tickers_json['data'][:25]\n",
        "        tickers_list = [x['ticker'] for x in data]\n",
        "        return tickers_list\n",
        "\n",
        "    def get_stock_data_from_marketstack(self):\n",
        "        \"\"\"\n",
        "        Connector # 1\n",
        "        Fetch historical stock data either from MarketStack API or a mock source.\n",
        "        \"\"\"\n",
        "        tickers_list = self.get_tickers()\n",
        "        tickers_str = ','.join(tickers_list)\n",
        "        if self.IS_MOCK:\n",
        "            marketstack_url = self.marketstack_apiurlgist\n",
        "        else:\n",
        "            marketstack_url = f'{self.marketstack_baseapiurl}&symbols={tickers_str}&date_from={self.datefrom}&date_to={self.dateto}'\n",
        "        stock_data = self.api_request(marketstack_url)['data']\n",
        "        df = pd.DataFrame(stock_data)\n",
        "        return df\n",
        "\n",
        "    def get_data_from_kaggle_df(self):\n",
        "        \"\"\"\n",
        "        Connector # 2\n",
        "        Load stock data from Kaggle dataset using KaggleHub.\n",
        "        \"\"\"\n",
        "        file_path = \"World-Stock-Prices-Dataset.csv\"\n",
        "        df = kagglehub.load_dataset(\n",
        "            KaggleDatasetAdapter.PANDAS,\n",
        "            \"nelgiriyewithana/world-stock-prices-daily-updating\",\n",
        "            file_path\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def get_data_from_local_csv(self):\n",
        "      \"\"\"\n",
        "      Connector #3\n",
        "      Load stock data from local CSV dataset.\n",
        "      \"\"\"\n",
        "      df = pd.read_csv(\"filtered_2024_stock_data.csv\")\n",
        "      return df\n",
        "\n",
        "    def get_data_from_mongodb(self):\n",
        "      \"\"\"\n",
        "      Connector #4\n",
        "      Load stock data from mongodb dataset.\n",
        "      \"\"\"\n",
        "      mongoclient = MongoClient('mongodb+srv://rayyanned_db1:rayyan123@cluster-ned.3frsh.mongodb.net/?retryWrites=true&w=majority&appName=Cluster-NED')\n",
        "      db = mongoclient['financial_stock']\n",
        "      collection = db['financial_stocks_data2023']\n",
        "\n",
        "      # Fetch all documents\n",
        "      data = list(collection.find({}))\n",
        "\n",
        "      # Convert to DataFrame\n",
        "      df = pd.DataFrame(data)\n",
        "\n",
        "      # Optionally drop the MongoDB \"_id\" field\n",
        "      if '_id' in df.columns:\n",
        "          df.drop('_id', axis=1, inplace=True)\n",
        "\n",
        "      return df\n",
        "\n",
        "    def get_data_from_github(self):\n",
        "      \"\"\"\n",
        "      Connector #5\n",
        "      Load stock data from github dataset.\n",
        "      \"\"\"\n",
        "      df = pd.read_csv('https://raw.githubusercontent.com/rayyanali00/ETL_Pipeline_SyedRayyanAli_DS042/refs/heads/main/filtered_2022_stock_data.csv')\n",
        "      return df\n",
        "\n",
        "    def check_for_null_values(self):\n",
        "        \"\"\"Check and return columns with null values in both datasets.\"\"\"\n",
        "        marketstack_df = self.get_stock_data_from_marketstack()\n",
        "        kaggle_df = self.get_data_from_kaggle_df()\n",
        "        local_csv = self.get_data_from_local_csv()\n",
        "        mongo_db = self.get_data_from_mongodb()\n",
        "\n",
        "        marketstack_nulls = marketstack_df.isnull().sum()\n",
        "        kaggle_nulls = kaggle_df.isnull().sum()\n",
        "\n",
        "        marketstack_null_cols = {col: count for col, count in marketstack_nulls.items() if count > 0}\n",
        "        kaggle_null_cols = {col: count for col, count in kaggle_nulls.items() if count > 0}\n",
        "\n",
        "        print(\"Null values in MarketStack DataFrame:\", marketstack_null_cols)\n",
        "        print(\"Null values in Kaggle DataFrame:\", kaggle_null_cols)\n",
        "\n",
        "        return {\n",
        "            \"marketstack_nulls\": marketstack_null_cols,\n",
        "            \"kaggle_nulls\": kaggle_null_cols\n",
        "        }\n",
        "\n",
        "    def handle_missing_values(self, kaggle_df):\n",
        "        \"\"\"Add capital gains feature (close - open) to Kaggle data.\"\"\"\n",
        "        kaggle_df['capital_gains'] = kaggle_df['close'] - kaggle_df['open']\n",
        "        return kaggle_df\n",
        "\n",
        "    def normalize_column_names(self, df):\n",
        "        \"\"\"Normalize column names to lowercase and underscores.\"\"\"\n",
        "        df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        return df\n",
        "\n",
        "    def validate_data(self, df):\n",
        "        \"\"\"Drop rows with invalid (negative) financial values.\"\"\"\n",
        "        numeric_cols = ['open', 'close', 'high', 'low', 'volume']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df = df[df[col] >= 0]\n",
        "        return df\n",
        "\n",
        "    def add_features(self, df):\n",
        "        \"\"\"Calculate daily return and volatility as new features.\"\"\"\n",
        "        if {'close', 'open', 'high', 'low'}.issubset(df.columns):\n",
        "            df['daily_return'] = (df['close'] - df['open']) / df['open']\n",
        "            df['volatility'] = df['high'] - df['low']\n",
        "        return df\n",
        "\n",
        "    def aggregate_data(self, df, date_col='date', groupby_col='symbol', company='name'):\n",
        "        \"\"\"Aggregate financial metrics by ticker and date.\"\"\"\n",
        "        df[date_col] = pd.to_datetime(df[date_col])\n",
        "        df['date_only'] = df[date_col].dt.date\n",
        "        grouped = df.groupby([groupby_col, 'date_only']).agg({\n",
        "            'open': 'mean',\n",
        "            'close': 'mean',\n",
        "            'high': 'max',\n",
        "            'low': 'min',\n",
        "            'volume': 'sum',\n",
        "            'daily_return': 'mean',\n",
        "            'volatility': 'mean'\n",
        "        }).reset_index()\n",
        "        return grouped\n",
        "\n",
        "    def standardize_timestamps(self, df, year, date_column='date'):\n",
        "      \"\"\"Convert date columns to datetime and filter data for year 2025.\"\"\"\n",
        "      df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
        "      df = df[df[date_column].dt.year == year]\n",
        "      return df\n",
        "\n",
        "    def merge_datasets(self, market_agg, kaggle_agg, local_agg, mongo_agg, github_agg):\n",
        "        \"\"\"Merge MarketStack and Kaggle data, dropping duplicates on symbol + date.\"\"\"\n",
        "        print(mongo_agg)\n",
        "        combined_df = pd.concat([market_agg, kaggle_agg, local_agg, mongo_agg, github_agg], ignore_index=True)\n",
        "        combined_df = combined_df.drop_duplicates(subset=['symbol', 'date_only'], keep='first')\n",
        "        return combined_df\n",
        "\n",
        "    def run_transformation(self):\n",
        "      \"\"\"\n",
        "      Runs the full ETL transformation pipeline:\n",
        "      1. Load data (including local and MongoDB data)\n",
        "      2. Normalize and validate\n",
        "      3. Feature engineering\n",
        "      4. Aggregation and merging\n",
        "      \"\"\"\n",
        "      # Step 1: Load\n",
        "      market_df = self.get_stock_data_from_marketstack() # Load MarketStack api\n",
        "      kaggle_df = self.get_data_from_kaggle_df() # Load Kaggle\n",
        "      local_df = self.get_data_from_local_csv()  # Load local CSV data\n",
        "      mongo_df = self.get_data_from_mongodb()    # Load MongoDB data\n",
        "      github_df = self.get_data_from_github() # Load Github csv\n",
        "\n",
        "      # Step 2: Standardize timestamps for each dataset (handling 'date' and 'Date' separately)\n",
        "      market_df = self.standardize_timestamps(market_df,2025, date_column='date')\n",
        "      kaggle_df = self.standardize_timestamps(kaggle_df,2025, date_column='Date')\n",
        "      local_df = self.standardize_timestamps(local_df,2024, date_column='Date')  # assuming local_df also uses 'date'\n",
        "      mongo_df = self.standardize_timestamps(mongo_df,2023, date_column='Date')  # assuming mongo_df also uses 'date'\n",
        "      github_df = self.standardize_timestamps(github_df,2022, date_column='Date')  # assuming mongo_df also uses 'date'\n",
        "\n",
        "      print('===========================')\n",
        "      print(mongo_df)\n",
        "      print('===========================')\n",
        "      print(local_df)\n",
        "\n",
        "      # Fix case issues in brand names for Kaggle data\n",
        "      kaggle_df['Brand_Name'] = kaggle_df['Brand_Name'].str.title()\n",
        "      local_df['Brand_Name'] = kaggle_df['Brand_Name'].str.title()\n",
        "      mongo_df['Brand_Name'] = kaggle_df['Brand_Name'].str.title()\n",
        "      github_df['Brand_Name'] = github_df['Brand_Name'].str.title()\n",
        "\n",
        "      # Step 3: Normalize column names for all datasets\n",
        "      market_df = self.normalize_column_names(market_df)\n",
        "      kaggle_df = self.normalize_column_names(kaggle_df)\n",
        "      local_df = self.normalize_column_names(local_df)\n",
        "      mongo_df = self.normalize_column_names(mongo_df)\n",
        "      github_df = self.normalize_column_names(github_df)\n",
        "\n",
        "      # Step 4: Handle missing data (you already have missing value handling for Kaggle)\n",
        "      kaggle_df = self.handle_missing_values(kaggle_df)\n",
        "      # You can apply similar missing value handling to other datasets (local_df, mongo_df)\n",
        "      # For simplicity, let's assume they all require the same missing values handling\n",
        "      local_df = self.handle_missing_values(local_df)\n",
        "\n",
        "      mongo_df = self.handle_missing_values(mongo_df)\n",
        "      github_df = self.handle_missing_values(github_df)\n",
        "\n",
        "      # Step 5: Validate data\n",
        "      market_df = self.validate_data(market_df)\n",
        "      kaggle_df = self.validate_data(kaggle_df)\n",
        "      local_df = self.validate_data(local_df)\n",
        "      mongo_df = self.validate_data(mongo_df)\n",
        "      github_df = self.validate_data(github_df)\n",
        "\n",
        "      # Step 6: Feature engineering\n",
        "      market_df = self.add_features(market_df)\n",
        "      kaggle_df = self.add_features(kaggle_df)\n",
        "      local_df = self.add_features(local_df)\n",
        "      mongo_df = self.add_features(mongo_df)\n",
        "      github_df = self.add_features(github_df)\n",
        "\n",
        "      # Step 7: Aggregate by date and symbol for all datasets\n",
        "      market_agg = self.aggregate_data(market_df)\n",
        "      kaggle_agg = self.aggregate_data(kaggle_df, date_col='date', groupby_col='ticker', company='brand_name')\n",
        "      local_agg = self.aggregate_data(local_df, date_col='date', groupby_col='ticker',company='brand_name')\n",
        "      mongo_agg = self.aggregate_data(mongo_df, date_col='date', groupby_col='ticker',company='brand_name')\n",
        "      github_agg = self.aggregate_data(github_df, date_col='date', groupby_col='ticker',company='brand_name')\n",
        "\n",
        "      kaggle_agg = kaggle_agg.rename(columns={'ticker': 'symbol'})\n",
        "      local_agg = local_agg.rename(columns={'ticker': 'symbol'})\n",
        "      mongo_agg = mongo_agg.rename(columns={'ticker': 'symbol'})\n",
        "      github_agg = github_agg.rename(columns={'ticker': 'symbol'})\n",
        "\n",
        "      # Step 8: Merge all datasets\n",
        "      merged_df = self.merge_datasets(market_agg, kaggle_agg, local_agg, mongo_agg, github_agg)\n",
        "\n",
        "      return merged_df\n",
        "\n",
        "    def load_data(self, df):\n",
        "        \"\"\"\n",
        "        Load final processed data to MongoDB.\n",
        "        Converts date to datetime to avoid BSON encoding errors.\n",
        "        \"\"\"\n",
        "        df['date_only'] = pd.to_datetime(df['date_only'])\n",
        "        mongoclient = MongoClient('mongodb+srv://rayyanned_db1:rayyan123@cluster-ned.3frsh.mongodb.net/?retryWrites=true&w=majority&appName=Cluster-NED')\n",
        "        db = mongoclient['financial_stock']\n",
        "        collection = db['stocksdata_new']\n",
        "\n",
        "        records = df.to_dict(orient='records')\n",
        "\n",
        "        if records:\n",
        "            collection.insert_many(records)\n",
        "            print(f\"Inserted {len(records)} documents into collection.\")\n",
        "        else:\n",
        "            print(\"No records to insert.\")\n"
      ],
      "metadata": {
        "id": "4_lTXQhJ2Phw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "etl_pipeline = ETLPipeline()\n",
        "df = etl_pipeline.run_transformation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DavAk9DtFOb2",
        "outputId": "384e7b79-7b64-46ea-e39b-128b2f46d74a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-74-f993520238e0>:54: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================\n",
            "                           Date        Open        High         Low  \\\n",
            "0     2023-12-29 05:00:00+00:00  104.910082  104.910082  104.712494   \n",
            "1     2023-12-29 05:00:00+00:00  259.604593  260.491279  258.707933   \n",
            "2     2023-12-29 05:00:00+00:00   89.146989   89.521485   88.536393   \n",
            "3     2023-12-29 05:00:00+00:00   79.320000   79.769997   76.855003   \n",
            "4     2023-12-29 05:00:00+00:00  374.630212  375.785989  372.119403   \n",
            "...                         ...         ...         ...         ...   \n",
            "15343 2023-01-03 05:00:00+00:00    8.170000    8.450000    7.890000   \n",
            "15344 2023-01-03 05:00:00+00:00  117.431224  118.362348  116.331699   \n",
            "15345 2023-01-03 05:00:00+00:00   82.134819   82.134819   82.134819   \n",
            "15346 2023-01-03 05:00:00+00:00  455.392038  456.187470  445.349544   \n",
            "15347 2023-01-03 05:00:00+00:00   66.300003   67.500000   62.501999   \n",
            "\n",
            "            Close      Volume  Dividends  Stock Splits Brand_Name Ticker  \\\n",
            "0      104.712494       300.0        0.0           0.0  bmw group  BAMXF   \n",
            "1      259.385406   4074600.0        0.0           0.0       visa      V   \n",
            "2       89.000443   2887622.0        0.0           0.0         3m    MMM   \n",
            "3       77.349998   8232300.0        0.0           0.0      block     SQ   \n",
            "4      374.670074  18723000.0        0.0           0.0  microsoft   MSFT   \n",
            "...           ...         ...        ...           ...        ...    ...   \n",
            "15343    8.120000  11626300.0        0.0           0.0    peloton   PTON   \n",
            "15344  117.629333   8124800.0        0.0           0.0       nike    NKE   \n",
            "15345   82.134819         0.0        0.0           0.0  bmw group  BAMXF   \n",
            "15346  450.698914   1978100.0        0.0           0.0     costco   COST   \n",
            "15347   64.639999  16416500.0        0.0           0.0      block     SQ   \n",
            "\n",
            "             Industry_Tag  Country  Capital Gains  \n",
            "0              automotive  germany            NaN  \n",
            "1                 finance      usa            NaN  \n",
            "2           manufacturing      usa            NaN  \n",
            "3      financial services      usa            NaN  \n",
            "4              technology      usa            NaN  \n",
            "...                   ...      ...            ...  \n",
            "15343             fitness      usa            NaN  \n",
            "15344             apparel      usa            NaN  \n",
            "15345          automotive  germany            NaN  \n",
            "15346              retail      usa            NaN  \n",
            "15347  financial services      usa            NaN  \n",
            "\n",
            "[15348 rows x 13 columns]\n",
            "===========================\n",
            "                           Date         Open         High          Low  \\\n",
            "0     2024-12-31 05:00:00+00:00    14.670000    14.680000    14.400000   \n",
            "1     2024-12-31 05:00:00+00:00   123.099998   123.550003   120.139999   \n",
            "2     2024-12-31 05:00:00+00:00   451.760010   453.109985   446.209991   \n",
            "3     2024-12-31 05:00:00+00:00   446.350006   448.500000   442.809998   \n",
            "4     2024-12-31 05:00:00+00:00   923.650024   924.739990   912.539978   \n",
            "...                         ...          ...          ...          ...   \n",
            "17243 2024-01-02 05:00:00+00:00    44.650002    44.959999    42.799999   \n",
            "17244 2024-01-02 05:00:00+00:00   156.930217   157.415584   156.197232   \n",
            "17245 2024-01-02 05:00:00+00:00   372.498019   374.530595   365.433851   \n",
            "17246 2024-01-02 05:00:00+00:00    18.280001    18.340000    17.700001   \n",
            "17247 2024-01-02 05:00:00+00:00  2278.449951  2285.000000  2237.560059   \n",
            "\n",
            "             Close      Volume  Dividends  Stock Splits Brand_Name Ticker  \\\n",
            "0        14.630000    257500.0        0.0           0.0   nintendo  NTDOY   \n",
            "1       120.790001  30118400.0        0.0           0.0        amd    AMD   \n",
            "2       447.380005    762300.0        0.0           0.0    spotify   SPOT   \n",
            "3       444.679993   2282800.0        0.0           0.0      adobe   ADBE   \n",
            "4       916.270020   1741100.0        0.0           0.0     costco   COST   \n",
            "...            ...         ...        ...           ...        ...    ...   \n",
            "17243    42.990002   7166700.0        0.0           0.0     roblox   RBLX   \n",
            "17244   156.722214    215500.0        0.0           0.0       lvmh  LVMUY   \n",
            "17245   369.518921  25258600.0        0.0           0.0  microsoft   MSFT   \n",
            "17246    17.770000   4142900.0        0.0           0.0   zoominfo     ZI   \n",
            "17247  2244.750000    199900.0        0.0           0.0   chipotle    CMG   \n",
            "\n",
            "       Industry_Tag Country  Capital Gains  \n",
            "0            gaming   japan            NaN  \n",
            "1        technology     usa            NaN  \n",
            "2             music     usa            NaN  \n",
            "3        technology     usa            NaN  \n",
            "4            retail     usa            NaN  \n",
            "...             ...     ...            ...  \n",
            "17243        gaming     usa            NaN  \n",
            "17244  luxury goods  france            NaN  \n",
            "17245    technology     usa            NaN  \n",
            "17246    technology     usa            NaN  \n",
            "17247          food     usa            NaN  \n",
            "\n",
            "[17248 rows x 13 columns]\n",
            "      symbol   date_only        open       close        high         low  \\\n",
            "0       AAPL  2023-01-03  129.726523  124.538658  130.343884  123.642480   \n",
            "1       AAPL  2023-01-04  126.350936  125.823189  128.113421  124.548628   \n",
            "2       AAPL  2023-01-05  126.589905  124.488869  127.227186  124.229979   \n",
            "3       AAPL  2023-01-06  125.474679  129.069336  129.736488  124.359434   \n",
            "4       AAPL  2023-01-09  129.915724  129.597076  132.843237  129.338186   \n",
            "...      ...         ...         ...         ...         ...         ...   \n",
            "15184     ZM  2023-12-22   72.120003   72.500000   72.839996   71.665001   \n",
            "15185     ZM  2023-12-26   72.610001   74.209999   74.349998   72.370003   \n",
            "15186     ZM  2023-12-27   74.300003   73.730003   74.769997   73.160004   \n",
            "15187     ZM  2023-12-28   73.769997   73.089996   74.000000   72.919998   \n",
            "15188     ZM  2023-12-29   72.980003   71.910004   73.598000   71.809998   \n",
            "\n",
            "            volume  daily_return  volatility  \n",
            "0      112117500.0     -0.039991    6.701404  \n",
            "1       89113600.0     -0.004177    3.564793  \n",
            "2       80962700.0     -0.016597    2.997207  \n",
            "3       87754700.0      0.028648    5.377053  \n",
            "4       70790800.0     -0.002453    3.505050  \n",
            "...            ...           ...         ...  \n",
            "15184    2383600.0      0.005269    1.174995  \n",
            "15185    2939900.0      0.022036    1.979996  \n",
            "15186    2392300.0     -0.007672    1.609993  \n",
            "15187    2330200.0     -0.009218    1.080002  \n",
            "15188    2425900.0     -0.014662    1.788002  \n",
            "\n",
            "[15189 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "etl_pipeline.load_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSJ3vgaZM38D",
        "outputId": "ce43e9cc-d311-43b1-e710-711fd2ffb955"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 49738 documents into collection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_daily_etl():\n",
        "    # Initialize your ETL pipeline class and run the transformation\n",
        "    pipeline = ETLPipeline()  # Use your class for ETL\n",
        "    df = pipeline.run_transformation()\n",
        "    pipeline.load_data(df)  # Load the processed data to MongoDB or other destination\n",
        "\n",
        "# Schedule the task to run every day at a specific time (e.g., 12:00 PM)\n",
        "schedule.every().day.at(\"12:00\").do(run_daily_etl)\n",
        "\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(60)  # Wait for the next scheduled task"
      ],
      "metadata": {
        "id": "q8eVoMEd6QNB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('final_output.csv')"
      ],
      "metadata": {
        "id": "BVLmO2cK6bB4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSvbnywH6cQ-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A839bPtL78KG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qd8bSqVq794v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utl35H6F-CQl"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "474PgeKf-GXU"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_pqmcFOpOR9X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}